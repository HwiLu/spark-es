{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "This notebook takes in the `gemini-feed` Kafka topic and produces to the `spark.out` topic a feed which includes the order price volume ratio and bid/ask liquidity for BTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://download.elastic.co/hadoop/elasticsearch-hadoop-6.1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip elasticsearch-hadoop-6.1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-6.1.1-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 459kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from elasticsearch)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set up Elasticsearch connection\n",
    "# by default we connect to elasticsearch:9200 \n",
    "# since we are running this notebook from the Spark-Node we need to use `elasticsearch` instead of `localhost`\n",
    "# as this is the name of the docker container running Elasticsearch\n",
    "es = Elasticsearch('elasticsearch:9200')\n",
    "\n",
    "# if the stream-test index exists, wipe it out and create a new one\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    es.indices.create('stream-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-class-path=elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkStreaming\")  \n",
    "sc.setLogLevel(\"WARN\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = ssc.textFileStream('sample/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sample(x):\n",
    "    data = json.loads(x)\n",
    "    data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S')\n",
    "    data['doc_id'] = data.pop('count')\n",
    "    return (data['doc_id'], json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = stream.map(lambda x: format_sample(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(rdd):\n",
    "        es_write_conf = {\n",
    "        \"es.nodes\" : 'elasticsearch',\n",
    "        \"es.port\" : '9200',\n",
    "        \"es.resource\" : 'stream-test/sample',\n",
    "        \"es.mapping.id\": \"doc_id\",\n",
    "        \"es.input.json\" : \"yes\"\n",
    "        }\n",
    "\n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "                path='-',\n",
    "                outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "                keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "                valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "                conf=es_write_conf)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-01-16 20:51:18\n",
      "-------------------------------------------\n",
      "(582, '{\"name\": \"Bilbo\", \"value\": 33, \"timestamp\": \"2018/01/16 20:51:16\", \"doc_id\": 582}')\n",
      "(583, '{\"name\": \"Bilbo\", \"value\": 10, \"timestamp\": \"2018/01/16 20:51:17\", \"doc_id\": 583}')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-16 20:51:21\n",
      "-------------------------------------------\n",
      "(584, '{\"name\": \"Legolas\", \"value\": 0, \"timestamp\": \"2018/01/16 20:51:18\", \"doc_id\": 584}')\n",
      "(586, '{\"name\": \"Legolas\", \"value\": 6, \"timestamp\": \"2018/01/16 20:51:20\", \"doc_id\": 586}')\n",
      "(585, '{\"name\": \"Samwise\", \"value\": 55, \"timestamp\": \"2018/01/16 20:51:19\", \"doc_id\": 585}')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-16 20:51:24\n",
      "-------------------------------------------\n",
      "(589, '{\"name\": \"Samwise\", \"value\": 21, \"timestamp\": \"2018/01/16 20:51:23\", \"doc_id\": 589}')\n",
      "(587, '{\"name\": \"Samwise\", \"value\": 84, \"timestamp\": \"2018/01/16 20:51:21\", \"doc_id\": 587}')\n",
      "(588, '{\"name\": \"Samwise\", \"value\": 57, \"timestamp\": \"2018/01/16 20:51:22\", \"doc_id\": 588}')\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-16 20:51:27\n",
      "-------------------------------------------\n",
      "(590, '{\"name\": \"Aragorn\", \"value\": 66, \"timestamp\": \"2018/01/16 20:51:24\", \"doc_id\": 590}')\n",
      "(592, '{\"name\": \"Aragorn\", \"value\": 100, \"timestamp\": \"2018/01/16 20:51:26\", \"doc_id\": 592}')\n",
      "(591, '{\"name\": \"Legolas\", \"value\": 13, \"timestamp\": \"2018/01/16 20:51:25\", \"doc_id\": 591}')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-01-16 20:51:30\n",
      "-------------------------------------------\n",
      "(594, '{\"name\": \"Aragorn\", \"value\": 4, \"timestamp\": \"2018/01/16 20:51:28\", \"doc_id\": 594}')\n",
      "(595, '{\"name\": \"Samwise\", \"value\": 86, \"timestamp\": \"2018/01/16 20:51:29\", \"doc_id\": 595}')\n",
      "(593, '{\"name\": \"Gandalf\", \"value\": 36, \"timestamp\": \"2018/01/16 20:51:27\", \"doc_id\": 593}')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = { \"es.resource\" : \"stream-test/sample\", \"es.nodes\" : \"elasticsearch\"}\n",
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('589',\n",
       "  {'doc_id': 589,\n",
       "   'name': 'Samwise',\n",
       "   'timestamp': '2018/01/16 20:51:23',\n",
       "   'value': 21}),\n",
       " ('585',\n",
       "  {'doc_id': 585,\n",
       "   'name': 'Samwise',\n",
       "   'timestamp': '2018/01/16 20:51:19',\n",
       "   'value': 55}),\n",
       " ('586',\n",
       "  {'doc_id': 586,\n",
       "   'name': 'Legolas',\n",
       "   'timestamp': '2018/01/16 20:51:20',\n",
       "   'value': 6}),\n",
       " ('587',\n",
       "  {'doc_id': 587,\n",
       "   'name': 'Samwise',\n",
       "   'timestamp': '2018/01/16 20:51:21',\n",
       "   'value': 84}),\n",
       " ('593',\n",
       "  {'doc_id': 593,\n",
       "   'name': 'Gandalf',\n",
       "   'timestamp': '2018/01/16 20:51:27',\n",
       "   'value': 36})]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 589,\n",
       "  'name': 'Samwise',\n",
       "  'timestamp': '2018/01/16 20:51:23',\n",
       "  'value': 21}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Learning Apach Spark\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(es_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(doc_id=589, name='Samwise', timestamp='2018/01/16 20:51:23', value=21)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Aragorn', count=3),\n",
       " Row(name='Gandalf', count=1),\n",
       " Row(name='Legolas', count=3),\n",
       " Row(name='Samwise', count=5),\n",
       " Row(name='Bilbo', count=2)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .groupby('name') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
